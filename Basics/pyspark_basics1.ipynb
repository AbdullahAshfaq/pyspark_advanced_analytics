{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\.pyenv\\pyenv-win\\versions\\3.7.3\\lib\\site-packages\\pyspark\\context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "| _c0|_c1|       _c2|\n",
      "+----+---+----------+\n",
      "|name|age|experience|\n",
      "|   A|  2|         5|\n",
      "|   B|  4|        10|\n",
      "|   C|  6|        20|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header',True).csv('test1.csv') # To tell that 1st row is header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|experience|\n",
      "+----+---+----------+\n",
      "|   A|  2|         5|\n",
      "|   B|  4|        10|\n",
      "|   C|  6|        20|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='A', age='2', experience='5'),\n",
       " Row(name='B', age='4', experience='10')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'name'>, Column<'age'>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark[0], df_pyspark[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check datatypes of columns\n",
    "2. Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header',True).csv('test1.csv')\n",
    "# print shape\n",
    "df_pyspark.count(), len(df_pyspark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()\n",
    "# Age and experience are int but it read as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default behavior is read all cols as string\n",
    "# We can inferschema using additional value\n",
    "\n",
    "df_pyspark = spark.read.option('header',True).csv('test1.csv', inferSchema=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way which is preferable\n",
    "df_pyspark = spark.read.csv('test1.csv', inferSchema=True, header=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'experience']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A|  2|\n",
      "|   B|  4|\n",
      "|   C|  6|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting column\n",
    "df_pyspark.select(['name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('experience', 'int')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+------------------+\n",
      "|summary|name|age|        experience|\n",
      "+-------+----+---+------------------+\n",
      "|  count|   3|  3|                 3|\n",
      "|   mean|null|4.0|11.666666666666666|\n",
      "| stddev|null|2.0| 7.637626158259733|\n",
      "|    min|   A|  2|                 5|\n",
      "|    max|   C|  6|                20|\n",
      "+-------+----+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+-----+\n",
      "|name|age|experience|exp+2|\n",
      "+----+---+----------+-----+\n",
      "|   A|  2|         5|    7|\n",
      "|   B|  4|        10|   12|\n",
      "|   C|  6|        20|   22|\n",
      "+----+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding Columns\n",
    "\n",
    "df_pyspark.withColumn(colName='exp+2', col=df_pyspark['experience']+2).show()\n",
    "# This is not inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|experience|\n",
      "+----+---+----------+\n",
      "|   A|  2|         5|\n",
      "|   B|  4|        10|\n",
      "|   C|  6|        20|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn(colName='exp+2', col=df_pyspark['experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+-----+\n",
      "|name|age|experience|exp+2|\n",
      "+----+---+----------+-----+\n",
      "|   A|  2|         5|    7|\n",
      "|   B|  4|        10|   12|\n",
      "|   C|  6|        20|   22|\n",
      "+----+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|experience|\n",
      "+----+---+----------+\n",
      "|   A|  2|         5|\n",
      "|   B|  4|        10|\n",
      "|   C|  6|        20|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('exp+2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+-----+\n",
      "|name|age|experience|exp+2|\n",
      "+----+---+----------+-----+\n",
      "|   A|  2|         5|    7|\n",
      "|   B|  4|        10|   12|\n",
      "|   C|  6|        20|   22|\n",
      "+----+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+-----+\n",
      "|name|age|experience|exp+2|\n",
      "+----+---+----------+-----+\n",
      "|   A|  2|         5|    7|\n",
      "|   B|  4|        10|   12|\n",
      "|   C|  6|        20|   22|\n",
      "+----+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumn(colName='exp+2', col=df_pyspark['experience']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"transform(age, lambdafunction((x_0 * 2), x_0))\" due to data type mismatch: Parameter 1 requires the \"ARRAY\" type, however \"age\" has the type \"INT\".;\n'Project [unresolvedalias(transform(age#1602, lambdafunction((lambda 'x_0 * 2), lambda 'x_0, false)), Some(org.apache.spark.sql.Column$$Lambda$3609/23224320@992e32))]\n+- Relation [name#1601,age#1602,experience#1603,salary#1604] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14716\\2237220051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mtransform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\abdul\\.pyenv\\pyenv-win\\versions\\3.7.3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3034\u001b[0m         \u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3035\u001b[0m         \"\"\"\n\u001b[1;32m-> 3036\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3037\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abdul\\.pyenv\\pyenv-win\\versions\\3.7.3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1323\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\abdul\\.pyenv\\pyenv-win\\versions\\3.7.3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"transform(age, lambdafunction((x_0 * 2), x_0))\" due to data type mismatch: Parameter 1 requires the \"ARRAY\" type, however \"age\" has the type \"INT\".;\n'Project [unresolvedalias(transform(age#1602, lambdafunction((lambda 'x_0 * 2), lambda 'x_0, false)), Some(org.apache.spark.sql.Column$$Lambda$3609/23224320@992e32))]\n+- Relation [name#1601,age#1602,experience#1603,salary#1604] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  transform\n",
    "df_pyspark.select(transform('age', lambda x: x*2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn(colName='has7Exp', col=if df_pyspark['experience']>7 then )\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-----+\n",
      "|name|age|exp|exp+2|\n",
      "+----+---+---+-----+\n",
      "|   A|  2|  5|    7|\n",
      "|   B|  4| 10|   12|\n",
      "|   C|  6| 20|   22|\n",
      "+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('experience','exp').show() # To rename 1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+-------+\n",
      "|Name|Age|exper|erper+2|\n",
      "+----+---+-----+-------+\n",
      "|   A|  2|    5|      7|\n",
      "|   B|  4|   10|     12|\n",
      "|   C|  6|   20|     22|\n",
      "+----+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newNames = ['Name','Age','exper','erper+2'] # Length of this should match column numbers\n",
    "df_pyspark.toDF(*newNames).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dropping row\n",
    "2. Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|name| age|experience|salary|\n",
      "+----+----+----------+------+\n",
      "|   A|   2|         5|1000.0|\n",
      "|   B|   4|        10| 150.0|\n",
      "|   C|   6|        20|2000.0|\n",
      "|   D|null|        24| 123.0|\n",
      "|   E|   8|      null| 300.2|\n",
      "|null|   2|      null| 150.0|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|name|age|experience|salary|\n",
      "+----+---+----------+------+\n",
      "|   A|  2|         5|1000.0|\n",
      "|   B|  4|        10| 150.0|\n",
      "|   C|  6|        20|2000.0|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|name| age|experience|salary|\n",
      "+----+----+----------+------+\n",
      "|   A|   2|         5|1000.0|\n",
      "|   B|   4|        10| 150.0|\n",
      "|   C|   6|        20|2000.0|\n",
      "|   D|null|        24| 123.0|\n",
      "|   E|   8|      null| 300.2|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how=all : If all values are null, drop row\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|name|age|experience|salary|\n",
      "+----+---+----------+------+\n",
      "|   A|  2|         5|1000.0|\n",
      "|   B|  4|        10| 150.0|\n",
      "|   C|  6|        20|2000.0|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how=any (default) : If any value in row is null, drop row\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|name| age|experience|salary|\n",
      "+----+----+----------+------+\n",
      "|   A|   2|         5|1000.0|\n",
      "|   B|   4|        10| 150.0|\n",
      "|   C|   6|        20|2000.0|\n",
      "|   D|null|        24| 123.0|\n",
      "|   E|   8|      null| 300.2|\n",
      "|null|   2|      null| 150.0|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|name| age|experience|salary|\n",
      "+----+----+----------+------+\n",
      "|   A|   2|         5|1000.0|\n",
      "|   B|   4|        10| 150.0|\n",
      "|   C|   6|        20|2000.0|\n",
      "|   D|null|        24| 123.0|\n",
      "|   E|   8|      null| 300.2|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresh=3 : Atleast 2 non-null values should be present to keep \n",
    "df_pyspark.na.drop(how='any', thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|name| age|experience|salary|\n",
      "+----+----+----------+------+\n",
      "|   A|   2|         5|1000.0|\n",
      "|   B|   4|        10| 150.0|\n",
      "|   C|   6|        20|2000.0|\n",
      "|   D|null|        24| 123.0|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset: Drop nan values only in a particular column\n",
    "df_pyspark.na.drop(how='any', subset=['experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------+------+\n",
      "|       name| age|experience|salary|\n",
      "+-----------+----+----------+------+\n",
      "|          A|   2|         5|1000.0|\n",
      "|          B|   4|        10| 150.0|\n",
      "|          C|   6|        20|2000.0|\n",
      "|          D|null|        24| 123.0|\n",
      "|          E|   8|      null| 300.2|\n",
      "|Missing Val|   2|      null| 150.0|\n",
      "+-----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put same value in all missing places\n",
    "df_pyspark.na.fill(value='Missing Val').show() # Nothing done in age/experience col because we can't put string in int column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|name|age|experience|salary|\n",
      "+----+---+----------+------+\n",
      "|   A|  2|         5|1000.0|\n",
      "|   B|  4|        10| 150.0|\n",
      "|   C|  6|        20|2000.0|\n",
      "|   D| -1|        24| 123.0|\n",
      "|   E|  8|        -1| 300.2|\n",
      "|null|  2|        -1| 150.0|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value=-1, subset=['age','experience']).show() # Nothing done because we can't put string in int column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['experience','age'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['experience','age']]\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+------------------+-----------+\n",
      "|name| age|experience|salary|experience_imputed|age_imputed|\n",
      "+----+----+----------+------+------------------+-----------+\n",
      "|   A|   2|         5|1000.0|                 5|          2|\n",
      "|   B|   4|        10| 150.0|                10|          4|\n",
      "|   C|   6|        20|2000.0|                20|          6|\n",
      "|   D|null|        24| 123.0|                24|          4|\n",
      "|   E|   8|      null| 300.2|                14|          8|\n",
      "|null|   2|      null| 150.0|                14|          2|\n",
      "+----+----+----------+------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+-----------------+-----------------+\n",
      "|summary|name|               age|       experience|           salary|\n",
      "+-------+----+------------------+-----------------+-----------------+\n",
      "|  count|   5|                 5|                4|                6|\n",
      "|   mean|null|               4.4|            14.75|620.5333333333333|\n",
      "| stddev|null|2.6076809620810595|8.770214744615247|753.6417362823444|\n",
      "|    min|   A|                 2|                5|            123.0|\n",
      "|    max|   E|                 8|               24|           2000.0|\n",
      "+-------+----+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
